{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word_level.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_uVyCFCGSYEx","executionInfo":{"status":"ok","timestamp":1608252722820,"user_tz":300,"elapsed":531,"user":{"displayName":"Bobby Pechu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQP4ZbPSnCpj9y4XtVFIwO_kUzXcMmpoyXfW9W7Q=s64","userId":"02367298297208878279"}},"outputId":"1641ac61-7bd5-4664-9654-262220df6ad6"},"source":["#model implemented from https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n","\n","import tensorflow as tf\n","\n","import numpy as np\n","import os\n","import time\n","import string\n","import glob\n","\n","\n","from numpy import array\n","from pickle import dump\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","from keras.models import load_model\n","from pickle import load\n","from random import randint\n","from keras.preprocessing.sequence import pad_sequences\n","\n","\n","# load\n","#path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n","#print(path_to_file)\n","text = open('politics.txt', 'rb').read().decode(encoding='utf-8')\n","print(f'Length of text: {len(text)} characters')\n","print(text[:250])\n","# The unique characters in the file\n","vocab = sorted(set(text))\n","print(f'{len(vocab)} unique characters')\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Length of text: 1119265 characters\n","Defection timed to hit tax pledge\n","\n","With impeccable and precisely-calculated timing, Tory defector Robert Jackson and his new Labour bosses have attempted to overshadow Michael Howard's latest announcement on taxation and spending.\n","\n","With just about ev\n","86 unique characters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TXOrPcfUVvVK","executionInfo":{"status":"ok","timestamp":1608252734647,"user_tz":300,"elapsed":631,"user":{"displayName":"Bobby Pechu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQP4ZbPSnCpj9y4XtVFIwO_kUzXcMmpoyXfW9W7Q=s64","userId":"02367298297208878279"}},"outputId":"b06cca26-d6a0-40a9-a20c-bfc74da5a43b"},"source":["#clean text\n","\n","def clean_text(text):\n","\n","  # replace '--' with a space ' '\n","  doc = text.replace('--', ' ')\n","\t# split into tokens by white space\n","  tokens = doc.split()\n","\t# remove punctuation from each token\n","  table = str.maketrans('', '', string.punctuation)\n","  tokens = [w.translate(table) for w in tokens]\n","\t# remove remaining tokens that are not alphabetic\n","  tokens = [word for word in tokens if word.isalpha()]\n","\t# make lower case\n","  tokens = [word.lower() for word in tokens]\n","\n","\n","  #add 'START_TOKEN' in front of the sentences\n","\n","\n","\n","\n","  return tokens\n","\n","\n","# clean document\n","tokens = clean_text(text)\n","print(tokens[:200])\n","print('Total Tokens: %d' % len(tokens))\n","print('Unique Tokens: %d' % len(set(tokens)))\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["['defection', 'timed', 'to', 'hit', 'tax', 'pledge', 'with', 'impeccable', 'and', 'preciselycalculated', 'timing', 'tory', 'defector', 'robert', 'jackson', 'and', 'his', 'new', 'labour', 'bosses', 'have', 'attempted', 'to', 'overshadow', 'michael', 'howards', 'latest', 'announcement', 'on', 'taxation', 'and', 'spending', 'with', 'just', 'about', 'everyone', 'in', 'westminster', 'now', 'working', 'towards', 'a', 'may', 'general', 'election', 'mr', 'howard', 'is', 'eager', 'to', 'map', 'out', 'some', 'clear', 'and', 'distinctive', 'policies', 'aimed', 'at', 'finally', 'shifting', 'the', 'tories', 'resolutely', 'depressing', 'poll', 'showings', 'the', 'big', 'idea', 'is', 'his', 'savings', 'on', 'waste', 'and', 'bureaucracy', 'which', 'mr', 'howard', 'has', 'pledged', 'to', 'plough', 'back', 'into', 'public', 'services', 'and', 'tax', 'cuts', 'and', 'it', 'was', 'virtually', 'certain', 'his', 'pledge', 'on', 'tax', 'cuts', 'was', 'meant', 'to', 'be', 'the', 'core', 'message', 'from', 'his', 'interview', 'on', 'the', 'bbc', 'ones', 'breakfast', 'with', 'frost', 'programme', 'he', 'and', 'his', 'shadow', 'chancellor', 'oliver', 'letwin', 'have', 'been', 'edging', 'towards', 'an', 'announcement', 'on', 'this', 'front', 'for', 'some', 'months', 'now', 'but', 'without', 'any', 'concrete', 'pledges', 'but', 'mr', 'howard', 'announced', 'that', 'of', 'the', 'he', 'has', 'earmarked', 'from', 'savings', 'will', 'be', 'used', 'to', 'plug', 'labours', 'claimed', 'financial', 'black', 'hole', 'with', 'any', 'left', 'over', 'going', 'to', 'tax', 'cuts', 'in', 'mr', 'letwins', 'first', 'budget', 'he', 'would', 'not', 'be', 'precise', 'but', 'there', 'are', 'already', 'suggestions', 'he', 'is', 'set', 'to', 'announce', 'lifting', 'the', 'threshold', 'on', 'income', 'tax']\n","Total Tokens: 186287\n","Unique Tokens: 11445\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HQq_miAQYhcF","executionInfo":{"status":"ok","timestamp":1608252739679,"user_tz":300,"elapsed":634,"user":{"displayName":"Bobby Pechu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQP4ZbPSnCpj9y4XtVFIwO_kUzXcMmpoyXfW9W7Q=s64","userId":"02367298297208878279"}},"outputId":"b143b9f8-2529-4692-da37-c9201089dc7c"},"source":["\n","# organize into sequences of tokens\n","length = 50 + 1\n","sequences = list()\n","for i in range(length, len(tokens)):\n","\t# select sequence of tokens\n","\tseq = tokens[i-length:i]\n","\t# convert into a line\n","\tline = ' '.join(seq)\n","\t# store\n","\tsequences.append(line)\n","print('Total Sequences: %d' % len(sequences))\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Total Sequences: 186236\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NkxRYYlQYtzZ","executionInfo":{"status":"ok","timestamp":1608252755966,"user_tz":300,"elapsed":508,"user":{"displayName":"Bobby Pechu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQP4ZbPSnCpj9y4XtVFIwO_kUzXcMmpoyXfW9W7Q=s64","userId":"02367298297208878279"}}},"source":["# save tokens to file, one dialog per line\n","def save_doc(lines, filename):\n","\tdata = '\\n'.join(lines)\n","\tfile = open(filename, 'w')\n","\tfile.write(data)\n","\tfile.close()\n","\n","\n","# save sequences to file\n","out_filename = 'politics_token_clean.txt'\n","save_doc(sequences, out_filename)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":761},"id":"LzzgHhU-ZqZe","executionInfo":{"status":"error","timestamp":1608252975636,"user_tz":300,"elapsed":106478,"user":{"displayName":"Bobby Pechu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQP4ZbPSnCpj9y4XtVFIwO_kUzXcMmpoyXfW9W7Q=s64","userId":"02367298297208878279"}},"outputId":"38b1c9fc-2dc8-4c6f-ed89-60c76ad986fc"},"source":["# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# load\n","in_filename = 'politics_token_clean.txt'\n","doc = load_doc(in_filename)\n","lines = doc.split('\\n')\n","\n","# integer encode sequences of words\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(lines)\n","sequences = tokenizer.texts_to_sequences(lines)\n","# vocabulary size\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# separate into input and output\n","sequences = array(sequences)\n","X, y = sequences[:,:-1], sequences[:,-1]\n","y = to_categorical(y, num_classes=vocab_size)\n","seq_length = X.shape[1]\n","\n","# define model\n","model = Sequential()\n","model.add(Embedding(vocab_size, 300, input_length=seq_length))\n","model.add(LSTM(512, return_sequences=True))\n","model.add(LSTM(512))\n","model.add(Dense(512, activation='relu'))\n","model.add(Dense(vocab_size, activation='softmax'))\n","print(model.summary())\n","# compile model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit model\n","model.fit(X, y, batch_size=128, epochs=50)\n","\n","# save the model to file\n","model.save('word_rnn.h5')\n","# save the tokenizer\n","dump(tokenizer, open('tokenizer.pkl', 'wb'))\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 50, 300)           3433800   \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 50, 512)           1665024   \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 512)               2099200   \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 512)               262656    \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 11446)             5871798   \n","=================================================================\n","Total params: 13,332,478\n","Trainable params: 13,332,478\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/50\n"," 997/1455 [===================>..........] - ETA: 37s - loss: 7.0761 - accuracy: 0.0631"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-d00cbc4b5f43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# save the model to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"N4r1JrclahNB","colab":{"base_uri":"https://localhost:8080/","height":388},"executionInfo":{"status":"error","timestamp":1608252841246,"user_tz":300,"elapsed":617,"user":{"displayName":"Bobby Pechu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQP4ZbPSnCpj9y4XtVFIwO_kUzXcMmpoyXfW9W7Q=s64","userId":"02367298297208878279"}},"outputId":"03d75a20-9c27-41cb-c337-f10ddaa15e8d"},"source":["# load\n","in_filename = 'politics_token_clean.txt'\n","doc = load_doc(in_filename)\n","lines = doc.split('\\n')\n","\n","\n","seq_length = len(lines[0].split()) - 1\n","\n","#load model \n","model=load_model('model.h5')\n","tokenizer = load(open('tokenizer.pkl', 'rb'))\n","\n","# generate a sequence from a language model\n","def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n","  result = list()\n","  in_text = seed_text\n","\t# generate a fixed number of words\n","  \n","  for _ in range(n_words):\n","\t\t# encode the text as integer\n","    encoded = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# truncate sequences to a fixed length\n","    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n","\t\t# predict probabilities for each word\n","    yhat = model.predict_classes(encoded, verbose=0)\n","\t\t# map predicted word index to word\n","    out_word = ''\n","    for word, index in tokenizer.word_index.items():\n","      if index == yhat:\n","        out_word = word\n","        break\n","\t\t# append to input\n","    in_text += ' ' + out_word \n","    result.append(out_word)\n","    if out_word=='ENDTOKEN':\n","      break;\n","  \n","  return ' '.join(result)\n","\n","\n","seed_text = lines[randint(0,len(lines))]\n","print(seed_text + '\\n')\n","\n","generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n","print(generated)"],"execution_count":7,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-96a1dba70183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: model.h5/{saved_model.pbtxt|saved_model.pb}"]}]},{"cell_type":"code","metadata":{"id":"UC36Fh2_V9Mp"},"source":["# load\r\n","in_filename = 'politics_token_clean.txt'\r\n","doc = load_doc(in_filename)\r\n","lines = doc.split('\\n')\r\n","\r\n","\r\n","seq_length = len(lines[0].split()) - 1\r\n","\r\n","#load model \r\n","model=load_model('model.h5')\r\n","tokenizer = load(open('tokenizer.pkl', 'rb'))\r\n","\r\n","# generate a sequence from a language model\r\n","def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\r\n","  result = list()\r\n","  in_text = seed_text\r\n","\t# generate a fixed number of words\r\n","  \r\n","  for _ in range(n_words):\r\n","\t\t# encode the text as integer\r\n","    encoded = tokenizer.texts_to_sequences([in_text])[0]\r\n","\t\t# truncate sequences to a fixed length\r\n","    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\r\n","\t\t# predict probabilities for each word\r\n","    yhat = model.predict_classes(encoded, verbose=0)\r\n","\t\t# map predicted word index to word\r\n","    out_word = ''\r\n","    for word, index in tokenizer.word_index.items():\r\n","      if index == yhat:\r\n","        out_word = word\r\n","        break\r\n","\t\t# append to input\r\n","    in_text += ' ' + out_word \r\n","    result.append(out_word)\r\n","    if out_word=='ENDTOKEN':\r\n","      break;\r\n","  \r\n","  return ' '.join(result)\r\n","\r\n","\r\n","seed_text = lines[randint(0,len(lines))]\r\n","print(seed_text + '\\n')\r\n","\r\n","generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\r\n","print(generated)"],"execution_count":null,"outputs":[]}]}